{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised learning (Regression)\n",
    "\n",
    "In supervised learning, the task is to infer hidden structure from labeled data, comprised of training examples $\\{(x_n, y_n)\\}$, Regression (typically) means the output y takes continuous values.\n",
    "\n",
    "We demonstrate how to do this in Edward with an example. The script is available [here](https://github.com/blei-lab/edward/blob/master/examples/bayesian_linear_regression_10d.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our toy dataset. They comprise of\n",
    "pairs of inputs $\\mathbf{x}_n\\in\\mathbb{R}^{10}$ and outputs\n",
    "$y_n\\in\\mathbb{R}$. They have a linear dependence with normally\n",
    "distributed noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, noise_std=0.5):\n",
    "  X = np.concatenate([np.linspace(0, 2, num=N / 2),\n",
    "                      np.linspace(6, 8, num=N / 2)])\n",
    "  y = 2.0 * X + 10 * norm.rvs(0, noise_std, size=N)\n",
    "  X = X.reshape((N, 1))\n",
    "  return X.astype(np.float32), y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Simulate training and test sets of $40$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 40  # num data points\n",
    "D = 1  # num features\n",
    "\n",
    "# DATA\n",
    "X_train, y_train = build_toy_dataset(N)\n",
    "X_test, y_test = build_toy_dataset(N)\n",
    "\n",
    "plt.scatter(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posit the model as Bayesian linear regression. For more details on the model, see the [Bayesian linear regression tutorial](http://edwardlib.org/tutorials/bayesian-linear-regression).\n",
    "\n",
    "Here we build the model in Edwardâ€™s native language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "# placeholder for the data\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "\n",
    "# priors on the latent variables\n",
    "w = Normal(mu=tf.zeros(D), sigma=tf.ones(D))\n",
    "b = Normal(mu=tf.zeros(1), sigma=tf.ones(1))\n",
    "\n",
    "#likelihood of the data\n",
    "y = Normal(mu=ed.dot(X, w) + b, sigma=tf.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Perform variational inference. Define the variational model to be a fully factorized normal across the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "qw = Normal(mu=tf.Variable(tf.random_normal([D])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "qb = Normal(mu=tf.Variable(tf.random_normal([1])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run mean-field variational inference for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {X: X_train, y: y_train}\n",
    "inference = ed.KLqp({w: qw, b: qb}, data)\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this case $\\text{MFVI}$ defaults to minimizing the\n",
    "$\\text{KL}(q\\|p)$ divergence measure using the reparameterization\n",
    "gradient.\n",
    "For more details on inference, see the $\\text{KL}(q\\|p)$ tutorial} -  [link](http://edwardlib.org/tutorials/klqp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critisicm \n",
    "\n",
    "Use point-based evaluation, and calculate the mean squared error for predictions on test data.\n",
    "\n",
    "We do this first by forming the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "y_post = ed.copy(y, {w: qw.mean(), b: qb.mean()})\n",
    "# This is equivalent to\n",
    "# y_post = Normal(mu=ed.dot(X, qw.mean()) + qb.mean(), sigma=tf.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate predictions from the posterior predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"test mean squared error: {:.3f}\".format(\n",
    "        ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model makes predictions with low mean squared error (relative to the magnitude of the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "Let's visualise the samples from the prior and the posterior.\n",
    "Let's first define a small helper method which does the visualisation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise(X_train, y_train, n_samples, w, b):\n",
    "    # first we sample from the distribution\n",
    "    w_samples = w.sample(n_samples).eval()\n",
    "    b_samples = b.sample(n_samples).eval()\n",
    "    # set up plotting\n",
    "    plt.scatter(X_train, y_train)\n",
    "    inputs = np.linspace(-1, 10, num=400, dtype=np.float32)\n",
    "    for ns in range(n_samples):\n",
    "        output = inputs * w_samples[ns] + b_samples[ns]\n",
    "        plt.plot(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# samples from the prior\n",
    "n_prior_samples = 10\n",
    "\n",
    "# we pass w and b as it is the prior\n",
    "visualise(X_train, y_train, n_prior_samples, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# samples from the posterior\n",
    "n_posterior_samples = 10\n",
    "\n",
    "# we pass qw and qb as it is the posterior\n",
    "visualise(X_train, y_train, n_posterior_samples, qw, qb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
